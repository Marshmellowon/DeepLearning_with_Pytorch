{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi_Layer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMkKu55CRvy/LmNImId2j8F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Marshmellowon/DeepLearning_with_Pytorch/blob/master/Multi_Layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1y93L_WTxe1i",
        "colab_type": "text"
      },
      "source": [
        "# XOR\n",
        "- 단층 퍼셉트론으로는 구현이 불가능하다. \n",
        "- XOR문제는 multi layer 퍼셉트론으로 구현이 가능하다.\n",
        "- input layer, hidden layer, output layer\n",
        "\n",
        "# Back Propagation\n",
        "- Back Propagation 그림\n",
        "- 입력 이 있을때 output을 구하는데 \n",
        "- loss에 대해 웨이트에 대한 미분값을 구함 loss값을 최소화 시킬수 있도록 웨이트를 없데이트 하는 것.\n",
        "> backpropagation은 김성훈 교수꺼 보고 오기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dqw3y725ylmt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
        "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n",
        "\n",
        "# nn Layers\n",
        "w1 = torch.Tensor(2,2).to(device)\n",
        "b1 = torch.Tensor(2).to(device)\n",
        "w2 = torch.Tensor(2,1).to(device)\n",
        "b2 = torch.Tensor(1).to(device)\n",
        "\n",
        "def sigmoid(x):\n",
        "  # sigmoid func\n",
        "  return 1.0 / (1.0 + torch.exp(-x))\n",
        "  # return torch.div(torch.tensor(1), torch.add(torch.tensor(1.0), torch.exp(-x)))\n",
        "\n",
        "def sigmoid_prime(x):\n",
        "  # derivative of the sigmoid func\n",
        "  return sigmoid(x) * (1 - sigmoid(x))\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJDZyITA0tH2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "outputId": "139c611d-9f08-4530-c74e-c057f565dbcb"
      },
      "source": [
        "for step in range(10001):\n",
        "  # forward\n",
        "  l1 = torch.add(torch.matmul(X, w1), b1)\n",
        "  print(\"l1 = \\n\", l1)\n",
        "\n",
        "  a1 = sigmoid(l1)\n",
        "  l2 = torch.add(torch.matmul(a1, w2), b2)\n",
        "  Y_pred = sigmoid(l2)\n",
        "\n",
        "  cost = -torch.mean(Y * torch.log(Y_pred) + (1 - Y) * torch.log(1 - Y_pred)) # binary cross entrophy\n",
        "\n",
        "  # back prop\n",
        "  # Loss derivative\n",
        "  d_Y_pred = (Y_pred - Y) / (Y_pred * (1.0 - Y_pred) + 1e-7) # binary cross entrophy를 미분한것 1e-7은 0으로 나눠지는것을 방지\n",
        "\n",
        "  # Layer 2\n",
        "  d_l2 = d_Y_pred * sigmoid_prime(l2) \n",
        "  d_b2 = d_l2\n",
        "  d_w2 = torch.matmul(torch.transpose(a1, 0, 1), d_b2) # transpose는 차원축을 지정할 수 있다. 두번째 인자값과 세번째 인자값을 바꿔서 한다.(5, 10) -> (10, 5)\n",
        "\n",
        "  print('d_b2 = \\n', d_b2)\n",
        "\n",
        "  # Layer 1\n",
        "  d_a1 = torch.matmul(d_b2, torch.transpose(w2, 0, 1)) # 왜 여기서 오류가 날까??? 일단 보류\n",
        "  d_l1 = d_a1 * sigmoid_prime(l1)\n",
        "  d_b1 = d_l1\n",
        "  d_w1 = torch.matmul(torch.transpose(X, 0, 1), d_b1)\n",
        "\n",
        "  # weight update\n",
        "  w1 = w1 - learning_rate * d_w1 # gradient decent\n",
        "  b2 = b1 - learning_rate * torch.mean(d_b1, 0)\n",
        "  w2 = w2 - learning_rate * d_w2\n",
        "  b2 = b2 - learning_rate * torch.mean(d_b2, 0)\n",
        "\n",
        "  if step % 100 == 0:\n",
        "    print(step, cost.item())\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "l1 = \n",
            " tensor([[2.2995e-35, 0.0000e+00],\n",
            "        [1.0000e+00, 0.0000e+00],\n",
            "        [4.5989e-35, 0.0000e+00],\n",
            "        [1.0000e+00, 0.0000e+00]])\n",
            "d_b2 = \n",
            " tensor([[ 0.5000],\n",
            "        [-0.5000],\n",
            "        [-0.5000],\n",
            "        [ 0.5000]])\n",
            "0 0.6931471824645996\n",
            "l1 = \n",
            " tensor([[2.2995e-35, 0.0000e+00],\n",
            "        [1.0000e+00, 0.0000e+00],\n",
            "        [4.6050e-35, 0.0000e+00],\n",
            "        [1.0000e+00, 0.0000e+00]])\n",
            "d_b2 = \n",
            " tensor([[ 0.5000,  0.5000],\n",
            "        [-0.5000, -0.5000],\n",
            "        [-0.5000, -0.5000],\n",
            "        [ 0.5000,  0.5000]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-c4a0d3c0204a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;31m# Layer 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0md_a1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_b2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 왜 여기서 오류가 날까???\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m   \u001b[0md_l1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_a1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msigmoid_prime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0md_b1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_l1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [4 x 2], m2: [1 x 2] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:41"
          ]
        }
      ]
    }
  ]
}